import operator
from typing import Annotated, Any, Optional

import streamlit as st
from dotenv import load_dotenv
from pydantic import BaseModel, Field

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langgraph.graph import END, StateGraph

# =========================
# åˆæœŸè¨­å®š
# =========================
st.set_page_config(page_title="è¦ä»¶å®šç¾© ç”Ÿæˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ", layout="wide")
st.title("ğŸ§© è¦ä»¶å®šç¾© ç”Ÿæˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆStreamlitç‰ˆï¼‰")
st.caption("LangChain + LangGraph ã§ã€ãƒšãƒ«ã‚½ãƒŠâ†’ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼â†’è©•ä¾¡â†’è¦ä»¶å®šç¾©ã‚’è‡ªå‹•ç”Ÿæˆ")

# .env èª­ã¿è¾¼ã¿ï¼ˆOPENAI_API_KEY ç­‰ï¼‰
load_dotenv()


# =========================
# ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«
# =========================
class Persona(BaseModel):
    name: str = Field(..., description="ãƒšãƒ«ã‚½ãƒŠã®åå‰")
    background: str = Field(..., description="ãƒšãƒ«ã‚½ãƒŠã®æŒã¤èƒŒæ™¯")


class Personas(BaseModel):
    personas: list[Persona] = Field(default_factory=list, description="ãƒšãƒ«ã‚½ãƒŠã®ãƒªã‚¹ãƒˆ")


class Interview(BaseModel):
    persona: Persona = Field(..., description="ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼å¯¾è±¡ã®ãƒšãƒ«ã‚½ãƒŠ")
    question: str = Field(..., description="ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã§ã®è³ªå•")
    answer: str = Field(..., description="ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã§ã®å›ç­”")


class InterviewResult(BaseModel):
    interviews: list[Interview] = Field(
        default_factory=list, description="ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼çµæœã®ãƒªã‚¹ãƒˆ"
    )


class EvaluationResult(BaseModel):
    reason: str = Field(..., description="åˆ¤æ–­ã®ç†ç”±")
    is_sufficient: bool = Field(..., description="æƒ…å ±ãŒååˆ†ã‹ã©ã†ã‹")


class InterviewState(BaseModel):
    user_request: str = Field(..., description="ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ")
    personas: Annotated[list[Persona], operator.add] = Field(
        default_factory=list, description="ç”Ÿæˆã•ã‚ŒãŸãƒšãƒ«ã‚½ãƒŠã®ãƒªã‚¹ãƒˆ"
    )
    interviews: Annotated[list[Interview], operator.add] = Field(
        default_factory=list, description="å®Ÿæ–½ã•ã‚ŒãŸã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã®ãƒªã‚¹ãƒˆ"
    )
    requirements_doc: str = Field(default="", description="ç”Ÿæˆã•ã‚ŒãŸè¦ä»¶å®šç¾©")
    iteration: int = Field(default=0, description="åå¾©å›æ•°")
    is_information_sufficient: bool = Field(
        default=False, description="æƒ…å ±ãŒååˆ†ã‹ã©ã†ã‹"
    )
    # è©•ä¾¡ç†ç”±ï¼ˆUIã§è¡¨ç¤ºã—ãŸã„ã®ã§ã‚¹ãƒ†ãƒ¼ãƒˆã«ã‚‚è¼‰ã›ã¦ãŠãï¼‰
    evaluation_reason: str = Field(default="", description="è©•ä¾¡ç†ç”±")


# =========================
# ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
# =========================
class PersonaGenerator:
    def __init__(self, llm: ChatOpenAI, k: int = 5):
        self.llm = llm.with_structured_output(Personas)
        self.k = k

    def run(self, user_request: str) -> Personas:
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    "ã‚ãªãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ç”¨ã®å¤šæ§˜ãªãƒšãƒ«ã‚½ãƒŠã‚’ä½œæˆã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚",
                ),
                (
                    "human",
                    f"ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«é–¢ã™ã‚‹ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ç”¨ã«ã€{self.k}äººã®å¤šæ§˜ãªãƒšãƒ«ã‚½ãƒŠã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n\n"
                    "ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆ: {user_request}\n\n"
                    "å„ãƒšãƒ«ã‚½ãƒŠã«ã¯åå‰ã¨ç°¡å˜ãªèƒŒæ™¯ã‚’å«ã‚ã¦ãã ã•ã„ã€‚å¹´é½¢ã€æ€§åˆ¥ã€è·æ¥­ã€æŠ€è¡“çš„å°‚é–€çŸ¥è­˜ã«ãŠã„ã¦å¤šæ§˜æ€§ã‚’ç¢ºä¿ã—ã¦ãã ã•ã„ã€‚",
                ),
            ]
        )
        chain = prompt | self.llm
        return chain.invoke({"user_request": user_request})


class InterviewConductor:
    def __init__(self, llm: ChatOpenAI):
        self.llm = llm

    def run(self, user_request: str, personas: list[Persona]) -> InterviewResult:
        questions = self._generate_questions(user_request=user_request, personas=personas)
        answers = self._generate_answers(personas=personas, questions=questions)
        interviews = self._create_interviews(personas=personas, questions=questions, answers=answers)
        return InterviewResult(interviews=interviews)

    def _generate_questions(self, user_request: str, personas: list[Persona]) -> list[str]:
        question_prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    "ã‚ãªãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼è¦ä»¶ã«åŸºã¥ã„ã¦é©åˆ‡ãªè³ªå•ã‚’ç”Ÿæˆã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚",
                ),
                (
                    "human",
                    "ä»¥ä¸‹ã®ãƒšãƒ«ã‚½ãƒŠã«é–¢é€£ã™ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«ã¤ã„ã¦ã€1ã¤ã®è³ªå•ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n\n"
                    "ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆ: {user_request}\n"
                    "ãƒšãƒ«ã‚½ãƒŠ: {persona_name} - {persona_background}\n\n"
                    "è³ªå•ã¯å…·ä½“çš„ã§ã€ã“ã®ãƒšãƒ«ã‚½ãƒŠã®è¦–ç‚¹ã‹ã‚‰é‡è¦ãªæƒ…å ±ã‚’å¼•ãå‡ºã™ã‚ˆã†ã«è¨­è¨ˆã—ã¦ãã ã•ã„ã€‚",
                ),
            ]
        )
        question_chain = question_prompt | self.llm | StrOutputParser()

        question_queries = [
            {
                "user_request": user_request,
                "persona_name": persona.name,
                "persona_background": persona.background,
            }
            for persona in personas
        ]
        return question_chain.batch(question_queries)

    def _generate_answers(self, personas: list[Persona], questions: list[str]) -> list[str]:
        answer_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", "ã‚ãªãŸã¯ä»¥ä¸‹ã®ãƒšãƒ«ã‚½ãƒŠã¨ã—ã¦å›ç­”ã—ã¦ã„ã¾ã™: {persona_name} - {persona_background}"),
                ("human", "è³ªå•: {question}"),
            ]
        )
        answer_chain = answer_prompt | self.llm | StrOutputParser()

        answer_queries = [
            {"persona_name": persona.name, "persona_background": persona.background, "question": question}
            for persona, question in zip(personas, questions)
        ]
        return answer_chain.batch(answer_queries)

    def _create_interviews(self, personas: list[Persona], questions: list[str], answers: list[str]) -> list[Interview]:
        return [
            Interview(persona=persona, question=question, answer=answer)
            for persona, question, answer in zip(personas, questions, answers)
        ]


class InformationEvaluator:
    def __init__(self, llm: ChatOpenAI):
        self.llm = llm.with_structured_output(EvaluationResult)

    def run(self, user_request: str, interviews: list[Interview]) -> EvaluationResult:
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    "ã‚ãªãŸã¯åŒ…æ‹¬çš„ãªè¦ä»¶æ–‡æ›¸ã‚’ä½œæˆã™ã‚‹ãŸã‚ã®æƒ…å ±ã®ååˆ†æ€§ã‚’è©•ä¾¡ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚",
                ),
                (
                    "human",
                    "ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¨ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼çµæœã«åŸºã¥ã„ã¦ã€åŒ…æ‹¬çš„ãªè¦ä»¶æ–‡æ›¸ã‚’ä½œæˆã™ã‚‹ã®ã«ååˆ†ãªæƒ…å ±ãŒé›†ã¾ã£ãŸã‹ã©ã†ã‹ã‚’åˆ¤æ–­ã—ã¦ãã ã•ã„ã€‚\n\n"
                    "ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆ: {user_request}\n\n"
                    "ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼çµæœ:\n{interview_results}",
                ),
            ]
        )
        chain = prompt | self.llm
        return chain.invoke(
            {
                "user_request": user_request,
                "interview_results": "\n".join(
                    f"ãƒšãƒ«ã‚½ãƒŠ: {i.persona.name} - {i.persona.background}\nè³ªå•: {i.question}\nå›ç­”: {i.answer}\n"
                    for i in interviews
                ),
            }
        )


class RequirementsDocumentGenerator:
    def __init__(self, llm: ChatOpenAI):
        self.llm = llm

    def run(self, user_request: str, interviews: list[Interview]) -> str:
        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", "ã‚ãªãŸã¯åé›†ã—ãŸæƒ…å ±ã«åŸºã¥ã„ã¦è¦ä»¶æ–‡æ›¸ã‚’ä½œæˆã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚"),
                (
                    "human",
                    "ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¨è¤‡æ•°ã®ãƒšãƒ«ã‚½ãƒŠã‹ã‚‰ã®ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼çµæœã«åŸºã¥ã„ã¦ã€è¦ä»¶æ–‡æ›¸ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\n\n"
                    "ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆ: {user_request}\n\n"
                    "ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼çµæœ:\n{interview_results}\n"
                    "è¦ä»¶æ–‡æ›¸ã«ã¯ä»¥ä¸‹ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å«ã‚ã¦ãã ã•ã„:\n"
                    "1. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦\n"
                    "2. ä¸»è¦æ©Ÿèƒ½\n"
                    "3. éæ©Ÿèƒ½è¦ä»¶\n"
                    "4. åˆ¶ç´„æ¡ä»¶\n"
                    "5. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼\n"
                    "6. å„ªå…ˆé †ä½\n"
                    "7. ãƒªã‚¹ã‚¯ã¨è»½æ¸›ç­–\n\n"
                    "å‡ºåŠ›ã¯å¿…ãšæ—¥æœ¬èªã§ãŠé¡˜ã„ã—ã¾ã™ã€‚\n\nè¦ä»¶æ–‡æ›¸:",
                ),
            ]
        )
        chain = prompt | self.llm | StrOutputParser()
        return chain.invoke(
            {
                "user_request": user_request,
                "interview_results": "\n".join(
                    f"ãƒšãƒ«ã‚½ãƒŠ: {i.persona.name} - {i.persona.background}\nè³ªå•: {i.question}\nå›ç­”: {i.answer}\n"
                    for i in interviews
                ),
            }
        )


class DocumentationAgent:
    def __init__(self, llm: ChatOpenAI, k: Optional[int] = None):
        self.persona_generator = PersonaGenerator(llm=llm, k=k or 5)
        self.interview_conductor = InterviewConductor(llm=llm)
        self.information_evaluator = InformationEvaluator(llm=llm)
        self.requirements_generator = RequirementsDocumentGenerator(llm=llm)
        self.graph = self._create_graph()

    def _create_graph(self) -> StateGraph:
        workflow = StateGraph(InterviewState)
        workflow.add_node("generate_personas", self._generate_personas)
        workflow.add_node("conduct_interviews", self._conduct_interviews)
        workflow.add_node("evaluate_information", self._evaluate_information)
        workflow.add_node("generate_requirements", self._generate_requirements)
        workflow.set_entry_point("generate_personas")
        workflow.add_edge("generate_personas", "conduct_interviews")
        workflow.add_edge("conduct_interviews", "evaluate_information")
        workflow.add_conditional_edges(
            "evaluate_information",
            lambda state: not state.is_information_sufficient and state.iteration < 5,
            {True: "generate_personas", False: "generate_requirements"},
        )
        workflow.add_edge("generate_requirements", END)
        return workflow.compile()

    def _generate_personas(self, state: InterviewState) -> dict[str, Any]:
        new_personas: Personas = self.persona_generator.run(state.user_request)
        return {
            "personas": new_personas.personas,
            "iteration": state.iteration + 1,
        }

    def _conduct_interviews(self, state: InterviewState) -> dict[str, Any]:
        # ç›´è¿‘ k äººåˆ†ã«å¯¾ã—ã¦è³ªå•ãƒ»å›ç­”ï¼ˆUIã® k ã¨æƒãˆã‚‹ï¼‰
        k = self.persona_generator.k
        target = state.personas[-k:] if k > 0 else state.personas
        new_interviews: InterviewResult = self.interview_conductor.run(state.user_request, target)
        return {"interviews": new_interviews.interviews}

    def _evaluate_information(self, state: InterviewState) -> dict[str, Any]:
        evaluation_result: EvaluationResult = self.information_evaluator.run(
            state.user_request, state.interviews
        )
        return {
            "is_information_sufficient": evaluation_result.is_sufficient,
            "evaluation_reason": evaluation_result.reason,
        }

    def _generate_requirements(self, state: InterviewState) -> dict[str, Any]:
        requirements_doc: str = self.requirements_generator.run(
            state.user_request, state.interviews
        )
        return {"requirements_doc": requirements_doc}

    def run_full(self, user_request: str) -> dict[str, Any]:
        initial_state = InterviewState(user_request=user_request)
        final_state = self.graph.invoke(initial_state)
        return {
            "requirements_doc": final_state["requirements_doc"],
            "personas": final_state["personas"],
            "interviews": final_state["interviews"],
            "evaluation_reason": final_state.get("evaluation_reason", ""),
            "iterations": final_state["iteration"],
            "is_information_sufficient": final_state["is_information_sufficient"],
        }


# =========================
# UIï¼ˆStreamlitï¼‰
# =========================
with st.sidebar:
    st.header("âš™ï¸ è¨­å®š")
    model = st.selectbox(
        "OpenAI ãƒ¢ãƒ‡ãƒ«",
        ["gpt-4o", "gpt-4.1", "gpt-4o-mini"],
        index=0,
    )
    temperature = st.slider("temperature", 0.0, 1.0, 0.0, 0.1)
    k = st.number_input("ç”Ÿæˆã™ã‚‹ãƒšãƒ«ã‚½ãƒŠäººæ•° k", min_value=1, max_value=10, value=5, step=1)
    st.caption("â€» ç’°å¢ƒå¤‰æ•° `OPENAI_API_KEY` ã‹ `.env` ã« API ã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")

st.subheader("ğŸ“ ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆ")
default_task = "ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³å‘ã‘ã®å¥åº·ç®¡ç†ã‚¢ãƒ—ãƒªã‚’é–‹ç™ºã—ãŸã„"
task = st.text_area("ä½œæˆã—ãŸã„ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ã¤ã„ã¦è¨˜è¼‰ã—ã¦ãã ã•ã„", value=default_task, height=120)

run = st.button("ğŸš€ ç”Ÿæˆã‚’å®Ÿè¡Œ", type="primary", disabled=not bool(task.strip()))

if run:
    try:
        with st.spinner("ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­..."):
            llm = ChatOpenAI(model=model, temperature=temperature)

        agent = DocumentationAgent(llm=llm, k=int(k))

        with st.spinner("ãƒšãƒ«ã‚½ãƒŠä½œæˆï½ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ï½è©•ä¾¡ï½è¦ä»¶å®šç¾©ã‚’ç”Ÿæˆä¸­..."):
            result = agent.run_full(user_request=task)

        # ========== å‡ºåŠ›è¡¨ç¤º ==========
        col1, col2 = st.columns([1, 1])

        with col1:
            st.markdown("### ğŸ‘¥ ç”Ÿæˆã•ã‚ŒãŸãƒšãƒ«ã‚½ãƒŠ")
            for idx, p in enumerate(result["personas"], start=1):
                with st.expander(f"Persona #{idx}: {p.name}", expanded=False):
                    st.write(p.background)

        with col2:
            st.markdown("### ğŸ—£ï¸ ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ï¼ˆè³ªå•ã¨å›ç­”ï¼‰")
            for idx, iv in enumerate(result["interviews"], start=1):
                with st.expander(f"Interview #{idx} - {iv.persona.name}", expanded=False):
                    st.markdown(f"**è³ªå•**: {iv.question}")
                    st.markdown(f"**å›ç­”**: {iv.answer}")

        st.markdown("### ğŸ§ª æƒ…å ±ååˆ†æ€§è©•ä¾¡")
        st.write("ååˆ†æ€§:", "âœ… ååˆ†" if result["is_information_sufficient"] else "âš ï¸ ä¸ååˆ†ï¼ˆæœ€å¤§5å›ã§æ‰“ã¡åˆ‡ã‚Šï¼‰")
        if result["evaluation_reason"]:
            st.info(result["evaluation_reason"])
        st.caption(f"åå¾©å›æ•°: {result['iterations']}")

        st.markdown("### ğŸ“„ ç”Ÿæˆã•ã‚ŒãŸè¦ä»¶å®šç¾©æ›¸")
        st.markdown(result["requirements_doc"])

        st.download_button(
            "â¬‡ï¸ è¦ä»¶å®šç¾©æ›¸ã‚’Markdownã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=result["requirements_doc"].encode("utf-8"),
            file_name="requirements.md",
            mime="text/markdown",
        )

    except Exception as e:
        st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.exception(e)
